# -*- coding: utf-8 -*-
"""Advanced Python prpject

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z_wWVrRxxK22uZVfTPY3oXeg1WWqV1Eo
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install alpha-vantage
# %pip install pandas-datareader
# %pip install sklearn

from pandas_datareader import data
import matplotlib.pyplot as plt
import pandas as pd
from datetime import date
from sklearn import preprocessing
import numpy as np
np.random.seed(123)
from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv1D, Flatten
from keras.optimizers import Adam
import tensorflow
tensorflow.random.set_seed(123)

history_points = 60

def get_dataset(panel_data):
    data = panel_data.copy()
    data['Close'] = data['Adj Close']
    data = data[['High',	'Low', 'Close', 'Volume', 'Open']]

    data_normalised = preprocessing.MinMaxScaler().fit_transform(data)

    # using the last {history_points} open close high low volume data points, predict the next open value
    ohlcv_histories_normalised = np.array([data_normalised[i:i + history_points].copy() for i in range(len(data_normalised) - history_points)])

    # get predicted value and reshape it
    next_day_open_values_normalised = np.array(data_normalised[:, -1][history_points:])
    next_day_open_values_normalised = np.expand_dims(next_day_open_values_normalised, -1)

    # real value
    next_day_open_values = np.array(data.iloc[:, -1][history_points:])
    next_day_open_values = np.expand_dims(next_day_open_values, -1)

    y_normaliser = preprocessing.MinMaxScaler()
    y_normaliser.fit(next_day_open_values)

    return ohlcv_histories_normalised, next_day_open_values_normalised, next_day_open_values, y_normaliser

start_date = '2010-01-01'
end_date = date.today().strftime("%Y-%m-%d")

# User pandas_reader.data.DataReader to load the desired data. As simple as that.
panel_data = data.DataReader('AAPL', 'yahoo', start_date, end_date)

ohlcv_histories, next_day_open_values, unscaled_y, y_normaliser = get_dataset(panel_data)

test_split = 0.9
n = int(ohlcv_histories.shape[0] * test_split)

ohlcv_train = ohlcv_histories[:n]
y_train = next_day_open_values[:n]

ohlcv_test = ohlcv_histories[n:]
y_test = next_day_open_values[n:]

unscaled_y_test = unscaled_y[n:]

model = Sequential()
model.add(Conv1D(128, 3, activation='relu', input_shape = (history_points, 5)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1, activation='linear'))

model.compile(optimizer=Adam(lr=0.00005), loss='mean_squared_logarithmic_error', metrics=['mse'])
history = model.fit(x=ohlcv_train, y=y_train, batch_size=32, epochs=500, shuffle=True, validation_split=0.2)

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

y_test_predicted = model.predict(ohlcv_test)
y_test_predicted = y_normaliser.inverse_transform(y_test_predicted)
y_predicted = model.predict(ohlcv_histories)
y_predicted = y_normaliser.inverse_transform(y_predicted)
real_mse = np.sqrt(np.mean(np.square(unscaled_y_test - y_test_predicted)))

print(real_mse)

real = plt.plot(unscaled_y_test[0:-1], label='real')
pred = plt.plot(y_test_predicted[0:-1], label='predicted')

#real = plt.plot(unscaled_y[start:end], label='real')
#pred = plt.plot(y_predicted[start:end], label='predicted')

plt.legend(['Real', 'Predicted'])

plt.show()

